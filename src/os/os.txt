os0thread:
/* Maximum number of threads which can be created in the program;
this is also the size of the wait slot array for MySQL threads which
can wait inside InnoDB */

#define OS_THREAD_MAX_N         srv_max_n_threads

note：
1. 线程数的大小取决于buffer pool的大小。
    /* Set the maximum number of threads which can wait for a semaphore
    inside InnoDB: this is the 'sync wait array' size, as well as the
    maximum number of threads that can wait in the 'srv_conc array' for
    their time to enter InnoDB. */

    if (srv_buf_pool_size >= 1000 * 1024 * 1024) {
        /* If buffer pool is less than 1000 MB,
        assume fewer threads. Also use only one
        buffer pool instance */
        srv_max_n_threads = 50000;

    } else if (srv_buf_pool_size >= 8 * 1024 * 1024) {

        srv_buf_pool_instances = 1;
        srv_max_n_threads = 10000;
    } else {
        srv_buf_pool_instances = 1;
        srv_max_n_threads = 1000;   /* saves several MB of memory,
                        especially in 64-bit
                        computers */
    }
2. 同时线程数的多少决定了初始化是slot数据的大小。
sync_primary_wait_array
sync_thread_level_arrays
srv_sys->threads
srv_mysql_table
3. 封装不同平台的thread函数。



/* Possible fixed priorities for threads */
#define OS_THREAD_PRIORITY_NONE         100
#define OS_THREAD_PRIORITY_BACKGROUND   1
#define OS_THREAD_PRIORITY_NORMAL       2
#define OS_THREAD_PRIORITY_ABOVE_NORMAL 3

note：
1. innodb中的调节线程优先级没有使用。
2. mysql服务器中的调节线程优先级也不使用。



os0proc:
note:
1. large_pages在linux上可以启动大内存页。不过事先要配置好hugetlb。只对buffer pool起作用，不包括common pool。
2. row_merge_build_indexes()为什么这里申请的内存很小还使用os_mem_alloc_large? 
3. linux下为什么要使用shmget，而不是mmap？ huge_tlb!
4. os_mem_alloc_large() -> ut_mem_alloc_large()


os0sync:
/** Native mutex */
typedef pthread_mutex_t     os_fast_mutex_t;
/** Native condition variable */
typedef pthread_cond_t      os_cond_t;

/** Operating system event */
typedef struct os_event_struct  os_event_struct_t;
/** Operating system event handle */
typedef os_event_struct_t*  os_event_t;

/** An asynchronous signal sent between threads */
struct os_event_struct {
    os_fast_mutex_t os_mutex;   /*!< this mutex protects the next
                    fields */
    ibool       is_set;     /*!< this is TRUE when the event is
                    in the signaled state, i.e., a thread
                    does not stop if it tries to wait for
                    this event */
    ib_int64_t  signal_count;   /*!< this is incremented each time
                    the event becomes signaled */
    os_cond_t   cond_var;   /*!< condition variable is used in
                    waiting for the event */
    UT_LIST_NODE_T(os_event_struct_t) os_event_list;
                    /*!< list of all created events */
};

/** Denotes an infinite delay for os_event_wait_time() */
#define OS_SYNC_INFINITE_TIME   ULINT_UNDEFINED

/** Return value of os_event_wait_time() when the time is exceeded */
#define OS_SYNC_TIME_EXCEEDED   1

/** Operating system mutex */
typedef struct os_mutex_struct  os_mutex_str_t;
/** Operating system mutex handle */
typedef os_mutex_str_t*     os_mutex_t;

/** Mutex protecting counts and the event and OS 'slow' mutex lists */
extern os_mutex_t   os_sync_mutex;

/** This is incremented by 1 in os_thread_create and decremented by 1 in
os_thread_exit */
extern ulint        os_thread_count;

extern ulint        os_event_count;
extern ulint        os_mutex_count;
extern ulint        os_fast_mutex_count;


/* Type definition for an operating system mutex struct */
struct os_mutex_struct{
        os_event_t      event;  /* Used by sync0arr.c for queing threads */
        void*           handle; /* OS handle to mutex */
        ulint           count;  /* we use this counter to check
                                that the same thread does not
                                recursively lock the mutex: we
                                do not assume that the OS mutex
                                supports recursive locking, though
                                NT seems to do that */
        UT_LIST_NODE_T(os_mutex_str_t) os_mutex_list;
                                /* list of all 'slow' OS mutexes created */
};

/* Mutex protecting counts and the lists of OS mutexes and events */
os_mutex_t      os_sync_mutex;
ibool           os_sync_mutex_inited    = FALSE;
ibool           os_sync_free_called     = FALSE;

/* This is incremented by 1 in os_thread_create and decremented by 1 in
os_thread_exit */
ulint   os_thread_count         = 0;

/* The list of all events created */
UT_LIST_BASE_NODE_T(os_event_struct_t)  os_event_list;

/* The list of all OS 'slow' mutexes */
UT_LIST_BASE_NODE_T(os_mutex_str_t)     os_mutex_list;


note:
1. os_mutex_list, os_event_list 保存着整个系统的系统mutex和event，其中每个mutex又包含一个event。
2. os_sync_mutex 保护整个系统的os_mutex_list, os_event_list. 同时自己也list中。(与mutex_list_mutex不同)
3. 注意os_event_struct中is_set， signal_count字段的含义。详细见os_event_wait_low()函数。
   is_set 表示event发生了。signal_count 用于防止event丢失的情况。
4. os_event_wait_low(),os_event_wait() 的区别在于后者忽略了事件丢失的情况。在调用os_event_wait_low()的时候如果reset_sig_count!=0,表示可以保证事件的发生.
5. 如果os_event_t 是在 os_mutex_create()中创建的，则释放的时候有os_mutex_free()来间接释放。并会调用os_event_free_internal(),避免循环加锁。
6. 在os_sync_free()中，采用优化的方式来释放。首先整体释放os_event_t，然后整体释放os_mutex_t。其实可以直接释放os_mutex_t，然后释放那些额外的os_event_t。
7. os_mutex_t 有什么用？为什么不直接使用os_fast_mutex_t,除了下面:
   os_event_t      event;  /* Used by sync0arr.c for queing threads */。 
8. struct mutex_struct, innodb自己实现的spin lock


os0file:
/* The aio array slot structure */
typedef struct os_aio_slot_struct       os_aio_slot_t;

struct os_aio_slot_struct{
        ibool           is_read;        /* TRUE if a read operation */
        ulint           pos;            /* index of the slot in the aio
                                        array */
        ibool           reserved;       /* TRUE if this slot is reserved */
        time_t          reservation_time;/* time when reserved */
        ulint           len;            /* length of the block to read or
                                        write */
        byte*           buf;            /* buffer used in i/o */
        ulint           type;           /* OS_FILE_READ or OS_FILE_WRITE */
        ulint           offset;         /* 32 low bits of file offset in
                                        bytes */
        ulint           offset_high;    /* 32 high bits of file offset */
        os_file_t       file;           /* file where to read or write */
        const char*     name;           /* file name or path */
        ibool           io_already_done;/* used only in simulated aio:
                                        TRUE if the physical i/o already
                                        made and only the slot message
                                        needs to be passed to the caller
                                        of os_aio_simulated_handle */
        fil_node_t*     message1;       /* message which is given by the */
        void*           message2;       /* the requester of an aio operation
                                        and which can be used to identify
                                        which pending aio operation was
                                        completed */
    #if defined(LINUX_NATIVE_AIO)
        struct iocb control;    /* Linux control block for aio */
        int     n_bytes;    /* bytes written/read. */
        int     ret;        /* AIO return code */
    #endif
};
/* The aio array structure */
typedef struct os_aio_array_struct      os_aio_array_t;

struct os_aio_array_struct{
        os_mutex_t      mutex;    /* the mutex protecting the aio array */
        os_event_t      not_full; /* The event which is set to the signaled
                                  state when there is space in the aio
                                  outside the ibuf segment */
        os_event_t      is_empty; /* The event which is set to the signaled
                                  state when there are no pending i/os
                                  in this array */
        ulint           n_slots;  /* Total number of slots in the aio array.
                                  This must be divisible by n_threads. */
        ulint           n_segments;/* Number of segments in the aio array of
                                  pending aio requests. A thread can wait
                                  separately for any one of the segments. */
        ulint           n_reserved;/* Number of reserved slots in the
                                  aio array outside the ibuf segment */
        os_aio_slot_t*  slots;    /* Pointer to the slots in the array */
#if defined(LINUX_NATIVE_AIO)
        io_context_t*       aio_ctx;
                    /* completion queue for IO. There is 
                    one such queue per segment. Each thread
                    will work on one ctx exclusively. */
        struct io_event*    aio_events;
                    /* The array to collect completed IOs.
                    There is one such event for each
                    possible pending IO. The size of the
                    array is equal to n_slots. */
#endif
};
/* Array of events used in simulated aio */
os_event_t*     os_aio_segment_wait_events      = NULL;

/* The aio arrays for non-ibuf i/o and ibuf i/o, as well as sync aio. These
are NULL when the module has not yet been initialized. */
static os_aio_array_t*  os_aio_read_array       = NULL;
static os_aio_array_t*  os_aio_write_array      = NULL;
static os_aio_array_t*  os_aio_ibuf_array       = NULL;
static os_aio_array_t*  os_aio_log_array        = NULL;
static os_aio_array_t*  os_aio_sync_array       = NULL;

static ulint    os_aio_n_segments       = ULINT_UNDEFINED;

/* If the following is TRUE, read i/o handler threads try to
wait until a batch of new read requests have been posted */
static ibool    os_aio_recommend_sleep_for_read_threads = FALSE;

ulint   os_n_file_reads         = 0;	//读文件的次数
ulint   os_bytes_read_since_printout = 0;
ulint   os_n_file_writes        = 0;	//写文件的次数
ulint   os_n_fsyncs             = 0;	
ulint   os_n_file_reads_old     = 0;
ulint   os_n_file_writes_old    = 0;
ulint   os_n_fsyncs_old         = 0;
time_t  os_last_printout;

ibool   os_has_said_disk_full   = FALSE;

/* The mutex protecting the following counts of pending I/O operations */
static os_mutex_t os_file_count_mutex;
ulint   os_file_n_pending_preads  = 0;  //当有pread，pwrite时。
ulint   os_file_n_pending_pwrites = 0;  //当有pread，pwrite时。
ulint   os_n_pending_writes = 0;        //
ulint   os_n_pending_reads = 0;         //

note：
1. 主要定义了跨平台的文件操作和异步i/o。
2. 根据os_aio_use_native_aio值来判断。
   linux下的异步队列：
   os_aio_ibuf_array
   os_aio_log_array
   os_aio_read_array
   os_aio_write_array
   以上每个队列中最多可以有8 * SRV_N_PENDING_IOS_PER_THREAD = 8 * 32个io请求。
   os_aio_sync_array
   以上每个队列中最多可以有SRV_MAX_N_PENDING_SYNC_IOS = 100个io请求。
3. os_aio_ibuf_array，os_aio_log_array 只能有一个segment，os_aio_read_array，os_aio_write_array的segments有线程数指定。  
   对于请求io文件的offset不同，会对应到不同的segment。
4. fil_io对os_aio进行了封装, 包括了同步io。
   
